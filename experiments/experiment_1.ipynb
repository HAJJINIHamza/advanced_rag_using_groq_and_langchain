{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG APP using Groq API and Langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hh\\projects\\advanced_rag_with_groq_and_langchain\\rag_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import getpass\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Langsmith api key\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rag with web loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load an llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello. What would you like to talk about or ask?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_llm = ChatGroq(\n",
    "                groq_api_key = os.environ[\"GROQ_API_KEY\"],\n",
    "                model=\"llama-3.1-8b-instant\",\n",
    "                temperature = 0.6\n",
    "                )\n",
    "#test\n",
    "groq_llm.invoke(\"Hello\").content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load an embedder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0627717524766922,\n",
       " 0.05495881289243698,\n",
       " 0.05216474458575249,\n",
       " 0.08579003810882568,\n",
       " -0.08274892717599869,\n",
       " -0.07457295060157776,\n",
       " 0.06855470687150955,\n",
       " 0.018396392464637756,\n",
       " -0.08201132714748383,\n",
       " -0.037384744733572006,\n",
       " 0.01212496217340231,\n",
       " 0.0035183527506887913,\n",
       " -0.004134288523346186,\n",
       " -0.043784428387880325,\n",
       " 0.021807368844747543,\n",
       " -0.0051027447916567326,\n",
       " 0.0195466298609972,\n",
       " -0.04234875738620758,\n",
       " -0.11035964637994766,\n",
       " 0.005424490198493004,\n",
       " -0.055734846740961075,\n",
       " 0.028052441775798798,\n",
       " -0.023158719763159752,\n",
       " 0.028481436893343925,\n",
       " -0.053709618747234344,\n",
       " -0.05260162800550461,\n",
       " 0.03393922746181488,\n",
       " 0.045388590544462204,\n",
       " 0.02371845208108425,\n",
       " -0.07312081009149551,\n",
       " 0.05477769672870636,\n",
       " 0.017047269269824028,\n",
       " 0.08136036992073059,\n",
       " -0.0028626802377402782,\n",
       " 0.011958098970353603,\n",
       " 0.07355853170156479,\n",
       " -0.0942375510931015,\n",
       " -0.0813620314002037,\n",
       " 0.04001542925834656,\n",
       " 0.0006920791929587722,\n",
       " -0.013393252156674862,\n",
       " -0.0545380525290966,\n",
       " 0.005151392892003059,\n",
       " -0.026139788329601288,\n",
       " 0.03680701181292534,\n",
       " -0.033959634602069855,\n",
       " 0.02109319157898426,\n",
       " 0.055948901921510696,\n",
       " 0.0577814057469368,\n",
       " -0.005418432876467705,\n",
       " -0.0684131532907486,\n",
       " -0.09023705124855042,\n",
       " -0.04286673292517662,\n",
       " 0.023652616888284683,\n",
       " 0.12149349600076675,\n",
       " 0.032392777502536774,\n",
       " -0.02265937626361847,\n",
       " -0.023281695321202278,\n",
       " 0.04886597394943237,\n",
       " -0.0593516044318676,\n",
       " -0.03406599536538124,\n",
       " 0.035962577909231186,\n",
       " -0.08115781843662262,\n",
       " -0.021979473531246185,\n",
       " 0.013187958858907223,\n",
       " -0.045855458825826645,\n",
       " -0.07041984796524048,\n",
       " -0.052603259682655334,\n",
       " -0.04802921786904335,\n",
       " -0.07454615831375122,\n",
       " -0.028731834143400192,\n",
       " 0.012831849977374077,\n",
       " -0.04292174056172371,\n",
       " 0.005366337485611439,\n",
       " -0.03909759968519211,\n",
       " 0.01591937057673931,\n",
       " 0.02024359069764614,\n",
       " 0.005266681779175997,\n",
       " 0.009581334888935089,\n",
       " -0.004767681937664747,\n",
       " 0.048087671399116516,\n",
       " -0.07309657335281372,\n",
       " -0.05034449324011803,\n",
       " 0.00965305045247078,\n",
       " 0.013496591709554195,\n",
       " 0.00043029605876654387,\n",
       " 0.01972522959113121,\n",
       " 0.06249546632170677,\n",
       " -0.01822717674076557,\n",
       " 0.031097164377570152,\n",
       " -0.08885379880666733,\n",
       " 0.04813241958618164,\n",
       " 0.026074284687638283,\n",
       " -0.01149405911564827,\n",
       " -0.09936173260211945,\n",
       " -0.027338199317455292,\n",
       " 0.07621530443429947,\n",
       " -0.0020965966396033764,\n",
       " -0.12358872592449188,\n",
       " 0.29806527495384216,\n",
       " 0.056306347250938416,\n",
       " 0.0784800574183464,\n",
       " 0.011043630540370941,\n",
       " 0.052400141954422,\n",
       " -0.0011880015954375267,\n",
       " 0.0009656676556915045,\n",
       " -0.05432724952697754,\n",
       " 0.02676473930478096,\n",
       " -0.012194235809147358,\n",
       " 0.010717098601162434,\n",
       " -3.719850064953789e-05,\n",
       " -0.03556570038199425,\n",
       " -0.032085489481687546,\n",
       " 0.01616874523460865,\n",
       " 0.08674965798854828,\n",
       " 0.018139412626624107,\n",
       " -0.011017515324056149,\n",
       " 0.04974030703306198,\n",
       " 0.022581057623028755,\n",
       " -2.2364429241861217e-05,\n",
       " 0.014939075335860252,\n",
       " -0.01117620151489973,\n",
       " 0.005659313872456551,\n",
       " -0.004114784765988588,\n",
       " 0.0031826752237975597,\n",
       " 0.023864958435297012,\n",
       " 0.01805107481777668,\n",
       " -5.850475056976865e-33,\n",
       " 0.05610702559351921,\n",
       " -0.03449847921729088,\n",
       " 0.033111777156591415,\n",
       " 0.1677228957414627,\n",
       " -0.031038103625178337,\n",
       " -0.004835406318306923,\n",
       " -0.06106496602296829,\n",
       " -0.0627109557390213,\n",
       " 0.02742655761539936,\n",
       " 0.06364010274410248,\n",
       " 0.043340399861335754,\n",
       " 0.06082940101623535,\n",
       " -0.018160806968808174,\n",
       " 0.042804181575775146,\n",
       " 0.01902417466044426,\n",
       " 0.08797792345285416,\n",
       " -0.03913282975554466,\n",
       " 0.04414266347885132,\n",
       " -0.0049410355277359486,\n",
       " 0.05105268582701683,\n",
       " -0.05431904271245003,\n",
       " 0.011173143051564693,\n",
       " 0.026699086651206017,\n",
       " 0.07509654015302658,\n",
       " 0.04869018495082855,\n",
       " -0.043542757630348206,\n",
       " 0.013338671997189522,\n",
       " -0.10262984037399292,\n",
       " 0.05215663090348244,\n",
       " 0.02223239839076996,\n",
       " -0.0297418013215065,\n",
       " -0.042270686477422714,\n",
       " 0.022984616458415985,\n",
       " 0.039574429392814636,\n",
       " 0.009094814769923687,\n",
       " 0.020864736288785934,\n",
       " 0.005092657636851072,\n",
       " -0.06276405602693558,\n",
       " -0.05021762102842331,\n",
       " -0.0052249194122850895,\n",
       " -0.05349817872047424,\n",
       " 0.029773417860269547,\n",
       " 0.021753674373030663,\n",
       " -0.020677991211414337,\n",
       " 0.023866193369030952,\n",
       " 0.005806677043437958,\n",
       " -0.0012138297315686941,\n",
       " 0.02266034297645092,\n",
       " 0.003156582359224558,\n",
       " 0.030960414558649063,\n",
       " -0.05296808108687401,\n",
       " 0.01867697201669216,\n",
       " -0.1401534080505371,\n",
       " 0.04147816449403763,\n",
       " -0.010278222151100636,\n",
       " -0.011593188159167767,\n",
       " -0.0334581583738327,\n",
       " -0.0505889393389225,\n",
       " 0.04686788097023964,\n",
       " 0.02471507526934147,\n",
       " 0.03349684178829193,\n",
       " 0.11170759052038193,\n",
       " -0.04034861922264099,\n",
       " -0.0042845155112445354,\n",
       " -0.08074373006820679,\n",
       " -0.056112196296453476,\n",
       " 0.03833863139152527,\n",
       " 0.011508354917168617,\n",
       " 0.0687379240989685,\n",
       " -0.03811732679605484,\n",
       " -0.04598833993077278,\n",
       " -0.016439544036984444,\n",
       " 0.024413006380200386,\n",
       " 0.01172210369259119,\n",
       " 0.00816856324672699,\n",
       " 0.03903687000274658,\n",
       " 0.02618655003607273,\n",
       " 0.010481804609298706,\n",
       " 0.04286019504070282,\n",
       " -0.04638158529996872,\n",
       " 0.006481151562184095,\n",
       " 0.04442337900400162,\n",
       " -0.018837662413716316,\n",
       " 0.007263423874974251,\n",
       " 0.05617957189679146,\n",
       " 0.053477998822927475,\n",
       " -0.02148381434381008,\n",
       " -0.08448570221662521,\n",
       " -0.012813979759812355,\n",
       " -0.03947557508945465,\n",
       " -0.05802649259567261,\n",
       " 0.03133400157094002,\n",
       " 0.045417338609695435,\n",
       " 0.011839071288704872,\n",
       " -0.01790621317923069,\n",
       " 4.586215086962156e-33,\n",
       " 0.13150787353515625,\n",
       " 0.07932811975479126,\n",
       " -0.09495959430932999,\n",
       " -0.024472150951623917,\n",
       " -0.05581621080636978,\n",
       " -0.009145205840468407,\n",
       " -0.03209042176604271,\n",
       " 0.11379319429397583,\n",
       " -0.14441624283790588,\n",
       " 0.008509675040841103,\n",
       " 0.030667023733258247,\n",
       " -0.012414874508976936,\n",
       " 0.07017046958208084,\n",
       " 0.02817986160516739,\n",
       " 0.0408221036195755,\n",
       " 0.01959081180393696,\n",
       " 0.1429504007101059,\n",
       " 0.05686995014548302,\n",
       " -0.04012209549546242,\n",
       " -0.01784392260015011,\n",
       " -0.06138443946838379,\n",
       " 0.0008760318742133677,\n",
       " -0.05490609258413315,\n",
       " -0.007102480158209801,\n",
       " -0.0004941619117744267,\n",
       " -0.013119377195835114,\n",
       " -0.0022669006139039993,\n",
       " 0.05856668949127197,\n",
       " -0.09971511363983154,\n",
       " -0.02553245984017849,\n",
       " 0.0785646066069603,\n",
       " 0.020517617464065552,\n",
       " -0.004633388016372919,\n",
       " 0.03030252642929554,\n",
       " 0.016833271831274033,\n",
       " 0.09143757820129395,\n",
       " 0.0167696550488472,\n",
       " -0.07980296015739441,\n",
       " 0.042510319501161575,\n",
       " -0.08439742028713226,\n",
       " -0.023859038949012756,\n",
       " 0.04701950401067734,\n",
       " 0.002414827700704336,\n",
       " 0.10931786149740219,\n",
       " -0.033905431628227234,\n",
       " -0.06402097642421722,\n",
       " -0.03772382810711861,\n",
       " 0.029074696823954582,\n",
       " -0.042449358850717545,\n",
       " 0.015861866995692253,\n",
       " -0.09060659259557724,\n",
       " -0.05576634779572487,\n",
       " 0.022608747705817223,\n",
       " 0.008000305853784084,\n",
       " -0.02246997505426407,\n",
       " 0.022058187052607536,\n",
       " -0.025199128314852715,\n",
       " 0.030339527875185013,\n",
       " 0.010110780596733093,\n",
       " -0.01854737289249897,\n",
       " 0.016989585012197495,\n",
       " 0.0763101875782013,\n",
       " 0.04162057116627693,\n",
       " 0.08752065151929855,\n",
       " -0.012092754244804382,\n",
       " 0.031135130673646927,\n",
       " -0.03219042718410492,\n",
       " 0.012798055075109005,\n",
       " 0.013538787141442299,\n",
       " -0.02975241281092167,\n",
       " 0.03678978979587555,\n",
       " -0.005846061743795872,\n",
       " -0.01105556171387434,\n",
       " 0.03867029398679733,\n",
       " -0.020741531625390053,\n",
       " -0.011265072040259838,\n",
       " -0.023767562583088875,\n",
       " -0.009984564036130905,\n",
       " -0.02310842275619507,\n",
       " 0.012011390179395676,\n",
       " -0.010644512251019478,\n",
       " 0.05136070400476456,\n",
       " -0.027734078466892242,\n",
       " -0.0002113774826284498,\n",
       " 0.0009463258320465684,\n",
       " -0.031582605093717575,\n",
       " 0.051229268312454224,\n",
       " 0.04432854428887367,\n",
       " -0.0037734205834567547,\n",
       " -0.04166802391409874,\n",
       " 0.02895454876124859,\n",
       " 0.033061299473047256,\n",
       " -0.015184327028691769,\n",
       " -0.00015761253598611802,\n",
       " -0.04412437975406647,\n",
       " -1.480350064753111e-08,\n",
       " -0.008695960976183414,\n",
       " 0.00013008390669710934,\n",
       " 0.016471125185489655,\n",
       " 0.059228941798210144,\n",
       " 0.04553879424929619,\n",
       " 0.03312689810991287,\n",
       " -0.09335853904485703,\n",
       " -0.03902721032500267,\n",
       " -0.020656133070588112,\n",
       " 0.012607487849891186,\n",
       " 0.0695396214723587,\n",
       " 0.07919184118509293,\n",
       " -0.07191089540719986,\n",
       " -0.004785018973052502,\n",
       " 0.08800482749938965,\n",
       " 0.047588545829057693,\n",
       " -0.05219278857111931,\n",
       " -0.0075255525298416615,\n",
       " -0.05771182104945183,\n",
       " -0.0929584950208664,\n",
       " -0.004501551389694214,\n",
       " 0.0011780423810705543,\n",
       " 0.024447273463010788,\n",
       " -0.06403973698616028,\n",
       " -0.0032274832483381033,\n",
       " -0.02796538919210434,\n",
       " -0.035407572984695435,\n",
       " 0.025036726146936417,\n",
       " -0.009852741844952106,\n",
       " 0.013252436183393002,\n",
       " 0.0011384859681129456,\n",
       " 0.17805421352386475,\n",
       " -0.036146294325590134,\n",
       " -0.007625850383192301,\n",
       " -0.03220127522945404,\n",
       " -0.04229317978024483,\n",
       " 0.004774483386427164,\n",
       " 0.028531866148114204,\n",
       " 0.0747230276465416,\n",
       " -0.014894123189151287,\n",
       " -0.05622069537639618,\n",
       " 0.027235785499215126,\n",
       " -0.011199400760233402,\n",
       " -0.10166788101196289,\n",
       " -0.019529294222593307,\n",
       " 0.027246439829468727,\n",
       " 0.035081006586551666,\n",
       " -0.08160559833049774,\n",
       " -0.0013377186842262745,\n",
       " -0.07635190337896347,\n",
       " -0.039957333356142044,\n",
       " 0.040781889110803604,\n",
       " 0.060128647834062576,\n",
       " 0.0725458487868309,\n",
       " 0.06967510282993317,\n",
       " 0.08909127116203308,\n",
       " 0.015957819297909737,\n",
       " -0.014873633161187172,\n",
       " -0.04674156755208969,\n",
       " -0.013411317951977253,\n",
       " 0.06513473391532898,\n",
       " 0.050905991345644,\n",
       " 0.051483530551195145,\n",
       " 0.00709216995164752]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = HuggingFaceEmbeddings( \n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "#test\n",
    "embedder.embed_query(\"Hello\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = len(embedder.embed_query(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store = FAISS(\n",
    "    embedding_function = embedder,\n",
    "    index = index,\n",
    "    docstore = InMemoryDocstore(),\n",
    "    index_to_docstore_id = {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing_extensions import TypedDict, List\n",
    "from langgraph.graph import StateGraph, START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "docs_splitted = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add docs to vectore store\n",
    "_ = vectore_store.add_documents(documents = docs_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt template\n",
    "prompt = \"\"\"\n",
    "Answer the following question : {question}\n",
    "using the relevant informations bellow :\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template = prompt,\n",
    "    input_features = [\"question\", \"context\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State of application \n",
    "class State(TypedDict):\n",
    "    question : str\n",
    "    context : List\n",
    "    answer : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "context = vectore_store.similarity_search(\"what is sensory memory ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve (state : State):\n",
    "    retrieved_docs = vectore_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    context = \"\\n\\n\".join([chunk.page_content for chunk in state[\"context\"]])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": context})\n",
    "    response = groq_llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"What is short term memory capacity ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What is short term memory capacity ?\"}, \n",
    "    stream_mode = \"updates\"\n",
    "):\n",
    "    print (f\"{step} \\n\\n ............ \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message, metadata in graph.stream(\n",
    "    {\"question\": \"What is short term memory capacity ?\"},\n",
    "    stream_mode = \"messages\"\n",
    "):\n",
    "    print (message.content, end = '|')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG with local documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import faiss\n",
    "from langchain_groq                         import ChatGroq\n",
    "from langchain_huggingface                  import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores                 import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.text_splitter                import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents               import Document\n",
    "from typing_extensions                      import TypedDict, List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM\n",
    "groq_llm = ChatGroq(\n",
    "    groq_api_key = os.environ[\"GROQ_API_KEY\"],\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name =\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector database \n",
    "embeddings_ex = embedder.embed_query(\"hi\")\n",
    "index = faiss.IndexFlatL2(len(embeddings_ex))\n",
    "vectore_store = FAISS(\n",
    "    embedding_function = embedder,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix directory \n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "with pdfplumber.open(\"documents/No More Mr. Nice Guy by Robert Glover.pdf\") as book:\n",
    "    for page in book.pages:\n",
    "        documents.append(page.extract_text())\n",
    "documents = \"\\n\".join(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_structured = Document(page_content=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100)\n",
    "documents_splitted = splitter.split_documents( [documents_structured])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add documents to our vectore store \n",
    "_ = vectore_store.add_documents(documents_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in vectore_store.similarity_search(query=\"How a person can get help from others ?\"):\n",
    "    print (source.page_content)\n",
    "    print (\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question : str\n",
    "    context : List\n",
    "    answer : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state : State):\n",
    "    retrieved_docs = vectore_store.similarity_search(state[\"question\"]) \n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Use informations bellow extracted from No more Mr. Nice Guy book written by Robert Glover, to answer the following question : \n",
    "    {question}\n",
    "    If you don't know the answer, don't make up one, simply say I don't know.\n",
    "    relevant informations : \n",
    "    {context}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template = prompt,\n",
    "               input_variables = [\"question\", \"context\"])\n",
    "\n",
    "def generate(state : State):\n",
    "    context = \"\\n\\n relevant information :\\n\".join(text.page_content for text in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": context})\n",
    "    response = groq_llm.invoke(messages)\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile my graph \n",
    "workflow = StateGraph(State).add_sequence([retrieve, generate])\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "book_rag = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = book_rag.invoke({\"question\": \"How a person can get help from others ?\"})\n",
    "response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"answer\"].content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval PART 2, conversational style RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "import pdfplumber\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph import END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_2 = InMemoryVectorStore(embedding=embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importer un document \n",
    "documents = []\n",
    "with pdfplumber.open(\"../documents/I-Will-Teach-You-to-Be-Rich-Book-Summary.pdf\") as book:\n",
    "    for page in book.pages:\n",
    "        documents.append(page.extract_text())\n",
    "documents = \"\\n\".join(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = Document(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents_splitted = splitter.split_documents([documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3e42c180-dbd3-4474-ad0c-cd6a36cfbdba',\n",
       " '8b71f44d-4698-4aa2-afba-77c6a738d103',\n",
       " '4b1b4aaa-d7b1-4582-8a32-24ad2a7406ec',\n",
       " 'e3148d8e-89e2-4d2f-a7e0-9e9fe25e46b9',\n",
       " '347fb7f0-c81f-4da0-abe8-48e14f3ac871',\n",
       " '62fbf805-4ea6-47ae-b809-2081a9a8e568',\n",
       " '4e8fe2f6-f7e3-430d-a10c-d66fd970b548',\n",
       " 'd3f51d2c-fb24-4b72-bd7f-87b5ae012bba',\n",
       " '4d23940d-28b8-4b64-bd2f-56525553c730',\n",
       " '93e343b2-00e8-460c-a011-fc6879c4a3d4',\n",
       " '2058e9c5-3bf3-4e84-94ae-ef2345dd40cd',\n",
       " '4fe681c2-fde4-44a2-bb7b-0baa0ecb71fc',\n",
       " 'b4dd504b-3046-4b9c-8fdf-91da71b2ac59',\n",
       " '6a408522-66fe-401f-9e2a-8cf69eb18624',\n",
       " '14d595c5-32cf-41a1-84b6-f53900087269',\n",
       " '56c03f14-980b-40b1-b59d-4f1c315fed08',\n",
       " '0d95543c-466b-46cf-a0a3-3259e85288f9',\n",
       " 'c020217b-25fc-4a36-b572-cb1f22f2cc7a',\n",
       " '7c98fae2-5785-4aa8-8400-1ba3c799d2df',\n",
       " 'ecaa130a-59ce-4973-9531-964b48ebdbf1',\n",
       " '3eda2473-5b11-4b2f-bf62-2c69f1f9d398',\n",
       " 'abfdb901-2832-4a10-9d33-3c07ee06a178',\n",
       " '3916d400-3c27-48d7-9955-c1bf5fce3440']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_2.add_documents(documents_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add langsmith tracing \n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Input Langsmith API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query:str):\n",
    "    \"\"\" Retrieve relevant informations relative to query \"\"\"\n",
    "    retrieved_docs = vector_store_2.similarity_search(query, k=2)\n",
    "    summary = \"\\n\\n\".join (\n",
    "        (f\"Source : {doc.metadata}. \\nContent : {doc.page_content}.\") for doc in retrieved_docs\n",
    "        )\n",
    "    return summary, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1 in our graph \n",
    "def query_or_respond(state:MessagesState):\n",
    "    \"\"\"Tool call for either formulate user query or respond directly\"\"\"\n",
    "    groq_llm_with_tools = groq_llm.bind_tools([retrieve])\n",
    "    response = groq_llm_with_tools.invoke(state[\"messages\"])\n",
    "    print (\"query_or_respond function :\", response)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node 2\n",
    "tools = ToolNode([retrieve])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node 3\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate an answer to the query\"\"\"\n",
    "    #Extract previous context\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else :\n",
    "            break\n",
    "    \n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    #Prompt \n",
    "    system_prompt = f\"\"\"\n",
    "        You are a Q&A assistant.\n",
    "        Use informations bellow to answer the given question.\n",
    "        If you don't know the answer, don't make up one, simply say I don't know.\n",
    "        relevant informations : \n",
    "        {docs_content}\n",
    "    \"\"\"\n",
    "    conversation_history = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_prompt)] + conversation_history\n",
    "    response = groq_llm.invoke(prompt)    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build nodes and edges of the graph\n",
    "#Initiate graph\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "# Nodes\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "# Edges\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END:END, \"tools\": \"tools\"}\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "rag_app = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_or_respond function : content='Nice to meet you, Hamza. Is there anything I can help you with today?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 218, 'total_tokens': 237, 'completion_time': 0.02831595, 'prompt_time': 0.011958099, 'queue_time': 0.087884563, 'total_time': 0.040274049}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--1216afb4-7088-46e8-b693-5a6bff23ad15-0' usage_metadata={'input_tokens': 218, 'output_tokens': 19, 'total_tokens': 237}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hello, my name is Hamza.', additional_kwargs={}, response_metadata={}, id='be844197-c09e-4d5c-aa15-c541e7296170'),\n",
       "  AIMessage(content='Nice to meet you, Hamza. Is there anything I can help you with today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 218, 'total_tokens': 237, 'completion_time': 0.02831595, 'prompt_time': 0.011958099, 'queue_time': 0.087884563, 'total_time': 0.040274049}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1216afb4-7088-46e8-b693-5a6bff23ad15-0', usage_metadata={'input_tokens': 218, 'output_tokens': 19, 'total_tokens': 237})]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the app \n",
    "input = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello, my name is Hamza.\"}]}\n",
    "rag_app.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Hamza.\n",
      "query_or_respond function : content=\"Hello Hamza, it's nice to meet you. Is there anything I can help you with today?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 218, 'total_tokens': 240, 'completion_time': 0.035025722, 'prompt_time': 0.012083691, 'queue_time': 0.087643364, 'total_time': 0.047109413}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--5e93bed3-aa0f-4815-92ff-28cb650dd4e9-0' usage_metadata={'input_tokens': 218, 'output_tokens': 22, 'total_tokens': 240}\n",
      "---------------------------\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Hamza, it's nice to meet you. Is there anything I can help you with today?\n"
     ]
    }
   ],
   "source": [
    "#Test the app \n",
    "input = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello, my name is Hamza.\"}]}\n",
    "for step in rag_app.stream(\n",
    "    input,\n",
    "    stream_mode = \"values\"\n",
    "):\n",
    "    print (\"---------------------------\")\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the difference between Lifecycle and index funds ?\n",
      "query_or_respond function : content='' additional_kwargs={'tool_calls': [{'id': 'qn7h4r8pt', 'function': {'arguments': '{\"query\":\"Difference between Lifecycle and index funds\"}', 'name': 'retrieve'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 220, 'total_tokens': 239, 'completion_time': 0.026616606, 'prompt_time': 0.011957453, 'queue_time': 0.088675201, 'total_time': 0.038574059}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--d39d37a2-684f-47bd-a440-86dbe2522ccd-0' tool_calls=[{'name': 'retrieve', 'args': {'query': 'Difference between Lifecycle and index funds'}, 'id': 'qn7h4r8pt', 'type': 'tool_call'}] usage_metadata={'input_tokens': 220, 'output_tokens': 19, 'total_tokens': 239}\n",
      "---------------------------\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (qn7h4r8pt)\n",
      " Call ID: qn7h4r8pt\n",
      "  Args:\n",
      "    query: Difference between Lifecycle and index funds\n",
      "---------------------------\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source : {}. \n",
      "Content : funds. (And those funds, in turn, will hold stocks from each of those areas.) In other words, your lifecycle\n",
      "fund will own many funds, which all own stocks and bonds. Lifecycle funds are different from index funds,\n",
      "which are also low cost but require you to own multiple funds if you want a comprehensive asset\n",
      "allocation. Multiple funds mean you have to rebalance your funds regularly, usually every year, which is\n",
      "a laborious process of redistributing your money to different investments so you get back to your target\n",
      "asset allocation. What a pain. Follow the steps below to pick your investment style and set up your\n",
      "automatic investment system.\n",
      "ACTION STEPS:\n",
      "•Figure out your investing style (30 minutes). Decide whether you want the simple investment options of a\n",
      "lifecycle fund, or more control (and the complexity) of index funds. I recommend a lifecycle fund as the\n",
      "85 Percent Solution..\n",
      "\n",
      "Source : {}. \n",
      "Content : and bonds). Generally, as you get older, you weight your portfolio towards less risky, more stable assets\n",
      "like bonds and invest less in riskier assets like stocks. It's okay to be a little riskier when you're younger as\n",
      "you have the gift of time on your side and this can help mitigate the losses that you may experience with\n",
      "riskier assets. Index funds can be a good option as they're low cost (making them cheaper than mutual\n",
      "funds) but are a little hands on as you'll need to invest in multiple funds in order to diversify your portfolio.\n",
      "The best option is a lifecycle fund which automatically diversifies your portfolio based on your age.\n",
      "Lifecycle funds are actually “funds-of-funds,” or collections made up of other funds, which offer automatic\n",
      "diversification. For example, a lifecycle fund might include large-cap, mid-cap, small-cap, and international\n",
      "funds. (And those funds, in turn, will hold stocks from each of those areas.) In other words, your lifecycle.\n",
      "---------------------------\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "According to the given content, the main difference between Lifecycle and index funds is the level of control and complexity involved.\n",
      "\n",
      "Lifecycle funds are a type of \"funds-of-funds\" that automatically diversify your portfolio based on your age, requiring little to no maintenance or rebalancing. They are a good option for those who want a simple investment solution.\n",
      "\n",
      "Index funds, on the other hand, are low-cost and require more control from the investor. You will need to invest in multiple funds to diversify your portfolio, which can be a laborious process of rebalancing your investments regularly.\n",
      "\n",
      "Source: {}\n"
     ]
    }
   ],
   "source": [
    "#Test the app \n",
    "input_2 = {\"messages\": [{\"role\": \"user\", \"content\": \"What is the difference between Lifecycle and index funds ?\"}]}\n",
    "for step in rag_app.stream(\n",
    "    input_2,\n",
    "    stream_mode = \"values\"\n",
    "):\n",
    "    print (\"---------------------------\")\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_or_respond function : content=\"Hello Hamza, it's nice to meet you. Is there something I can help you with today?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 218, 'total_tokens': 240, 'completion_time': 0.028971967, 'prompt_time': 0.0120202, 'queue_time': 0.086725526, 'total_time': 0.040992167}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--9efc450d-fae3-40dc-9866-314f9100651a-0' usage_metadata={'input_tokens': 218, 'output_tokens': 22, 'total_tokens': 240}\n"
     ]
    }
   ],
   "source": [
    "#Test the app \n",
    "input = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello, my name is Hamza.\"}]}\n",
    "response = rag_app.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Hamza, it's nice to meet you. Is there something I can help you with today?\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
